{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPzvg1Nuomv4/kdXYkJsR7G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Earendil961/DM_project/blob/after_fix/part_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xnbMbves9srU"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "from part_1 import (create_gd, max_degree, number_of_connectivity_components,\n",
        "                    size_max_clique, size_max_independent_set)\n",
        "\n",
        "\n",
        "def extract_multiple_features(samples, n, k_or_d, graph_type):\n",
        "    features = []\n",
        "    if graph_type == \"stud\":\n",
        "        graph = create_gd(samples, n, k_or_d)\n",
        "        features.append(max_degree(n, graph))\n",
        "        features.append(size_max_independent_set(n, graph))\n",
        "    else:\n",
        "        graph = create_gd(samples, n, k_or_d)\n",
        "        features.append(number_of_connectivity_components(graph))\n",
        "        features.append(size_max_clique(graph))\n",
        "    return features\n",
        "\n",
        "\n",
        "def build_classifier(n, k_or_d, dist1, dist2, type1, iterations=50):\n",
        "    X = []\n",
        "    y = []\n",
        "\n",
        "    for i in range(iterations):\n",
        "        if dist1 == \"stud\":\n",
        "            samples1 = np.random.standard_t(df=3, size=n)\n",
        "        elif dist1 == \"lap\":\n",
        "            samples1 = np.random.laplace(loc=0, scale=0.70710678118, size=n)\n",
        "        elif dist1 == \"weib\":\n",
        "            samples1 = np.random.weibull(a=1 / 2, size=n) * 0.31622776601\n",
        "        elif dist1 == \"exp\":\n",
        "            samples1 = np.random.exponential(scale=1, size=n)\n",
        "\n",
        "        features1 = extract_multiple_features(samples1, n, k_or_d, dist1)\n",
        "        X.append(features1)\n",
        "        y.append(0)\n",
        "\n",
        "        if dist2 == \"stud\":\n",
        "            samples2 = np.random.standard_t(df=3, size=n)\n",
        "        elif dist2 == \"lap\":\n",
        "            samples2 = np.random.laplace(loc=0, scale=0.70710678118, size=n)\n",
        "        elif dist2 == \"weib\":\n",
        "            samples2 = np.random.weibull(a=1 / 2, size=n) * 0.31622776601\n",
        "        elif dist2 == \"exp\":\n",
        "            samples2 = np.random.exponential(scale=1, size=n)\n",
        "\n",
        "        features2 = extract_multiple_features(samples2, n, k_or_d, dist1)\n",
        "        X.append(features2)\n",
        "        y.append(1)\n",
        "\n",
        "    if dist1 == \"stud\":\n",
        "        feature_names = [\"max_degree\", \"size_max_independent_set\"]\n",
        "    else:\n",
        "        number = \"number_of_connectivity_components\"\n",
        "        feature_names = [number, \"size_max_clique\"]\n",
        "    df = pd.DataFrame(X, columns=feature_names)\n",
        "    df[\"target\"] = y\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.3, random_state=42\n",
        "    )\n",
        "\n",
        "    clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    if type1 == \"har_analyse\":\n",
        "        for name, importance in zip(feature_names, clf.feature_importances_):\n",
        "            print(f\"{name}: {importance:.4f}\")\n",
        "    return clf, df\n",
        "\n",
        "\n",
        "def analyze_feature_importance_vs_n(n_range, k_or_d, dist1, dist2):\n",
        "    importance_results = {}\n",
        "\n",
        "    for n in n_range:\n",
        "        n_name = \"n_analyse\"\n",
        "        clf, df = build_classifier(n, k_or_d, dist1, dist2, n_name, 50)\n",
        "        importance_results[n] = clf.feature_importances_\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    for feature_idx in range(len(clf.feature_importances_)):\n",
        "        importances = [importance_results[n][feature_idx] for n in n_range]\n",
        "        plt.plot(n_range, importances, label=f\"Признак {feature_idx}\")\n",
        "\n",
        "    plt.xlabel(\"Размер n\")\n",
        "    plt.ylabel(\"Важность признака\")\n",
        "    plt.title(\"Зависимость важности от n\")\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def t_classifier_1(classifier, dist, n=50):\n",
        "    targets = [1] * n + [0] * n\n",
        "    true_true = 0\n",
        "    true_false = 0\n",
        "    false_true = 0\n",
        "    false_false = 0\n",
        "    for i in range(n):\n",
        "        samples = np.random.standard_t(df=3, size=n)\n",
        "        graph = create_gd(samples, n, dist)\n",
        "        a = max_degree(n, graph)\n",
        "        b = size_max_independent_set(n, graph)\n",
        "        predict = classifier(a, b)\n",
        "        if predict == targets[i]:\n",
        "            if targets[i] == 1:\n",
        "                true_true += 1\n",
        "            else:\n",
        "                true_false += 1\n",
        "        else:\n",
        "            if targets[i] == 1:\n",
        "                false_true += 1\n",
        "            else:\n",
        "                false_false += 1\n",
        "    for i in range(n, 2 * n):\n",
        "        samples = np.random.laplace(loc=0, scale=0.70710678118, size=n)\n",
        "        graph = create_gd(samples, n, dist)\n",
        "        a = max_degree(n, graph)\n",
        "        b = size_max_independent_set(n, graph)\n",
        "        predict = classifier(a, b)\n",
        "        if predict == targets[i]:\n",
        "            if targets[i] == 1:\n",
        "                true_true += 1\n",
        "            else:\n",
        "                true_false += 1\n",
        "        else:\n",
        "            if targets[i] == 1:\n",
        "                false_true += 1\n",
        "            else:\n",
        "                false_false += 1\n",
        "    print(\"Ошибка первого рода: \", true_false / 2 * n)\n",
        "    print(\"Мощность: \", true_true / 2 * n)\n",
        "    print(\"Точность: \", (true_true + false_false) / 2 * n)\n",
        "    accuracy = (true_true + false_false) / 2 * n\n",
        "    return [true_false / 2 * n, true_true / 2 * n, accuracy]\n",
        "\n",
        "\n",
        "def t_classifier_2(classifier, dist, n=50):\n",
        "    targets = [1] * n + [0] * n\n",
        "    true_true = 0\n",
        "    true_false = 0\n",
        "    false_true = 0\n",
        "    false_false = 0\n",
        "    for i in range(n):\n",
        "        samples = np.random.weibull(a=1 / 2, size=n) * 0.31622776601\n",
        "        graph = create_gd(samples, n, dist)\n",
        "        a = number_of_connectivity_components(graph)\n",
        "        b = size_max_clique(graph)\n",
        "        predict = classifier(a, b)\n",
        "        if predict == targets[i]:\n",
        "            if targets[i] == 1:\n",
        "                true_true += 1\n",
        "            else:\n",
        "                true_false += 1\n",
        "        else:\n",
        "            if targets[i] == 1:\n",
        "                false_true += 1\n",
        "            else:\n",
        "                false_false += 1\n",
        "    for i in range(n, 2 * n):\n",
        "        samples = np.random.exponential(scale=1, size=n)\n",
        "        graph = create_gd(samples, n, dist)\n",
        "        a = number_of_connectivity_components(graph)\n",
        "        b = size_max_clique(graph)\n",
        "        predict = classifier(a, b)\n",
        "        if predict == targets[i]:\n",
        "            if targets[i] == 1:\n",
        "                true_true += 1\n",
        "            else:\n",
        "                true_false += 1\n",
        "        else:\n",
        "            if targets[i] == 1:\n",
        "                false_true += 1\n",
        "            else:\n",
        "                false_false += 1\n",
        "    print(\"Ошибка первого рода: \", true_false / 2 * n)\n",
        "    print(\"Мощность: \", true_true / 2 * n)\n",
        "    print(\"Точность: \", (true_true + false_false) / 2 * n)\n",
        "    accuracy = (true_true + false_false) / 2 * n\n",
        "    return [true_false / 2 * n, true_true / 2 * n, accuracy]\n",
        "\n",
        "\n",
        "def Analyze_of_metric(n_values, k_or_d, dist1, dist2, classifier_name):\n",
        "    results = []\n",
        "    log_params = {\"max_iter\": 1000, \"random_state\": 42}\n",
        "    classifiers = {\n",
        "        \"Дерево\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "        \"Логистическая регрессия\": LogisticRegression(**log_params),\n",
        "        \"K-ближайших соседей\": KNeighborsClassifier(n_neighbors=5),\n",
        "    }\n",
        "    selected_clf = classifiers[classifier_name]\n",
        "\n",
        "    for n in n_values:\n",
        "        X = []\n",
        "        y = []\n",
        "        for i in range(50):\n",
        "            if dist1 == \"stud\":\n",
        "                samples1 = np.random.standard_t(df=3, size=n)\n",
        "            elif dist1 == \"lap\":\n",
        "                beta = 0.70710678118\n",
        "                samples1 = np.random.laplace(loc=0, scale=beta, size=n)\n",
        "            elif dist1 == \"weib\":\n",
        "                samples1 = np.random.weibull(a=1 / 2, size=n) * 0.31622776601\n",
        "            elif dist1 == \"exp\":\n",
        "                samples1 = np.random.exponential(scale=1, size=n)\n",
        "\n",
        "            features1 = extract_multiple_features(samples1, n, k_or_d, dist1)\n",
        "            X.append(features1)\n",
        "            y.append(0)\n",
        "\n",
        "            if dist2 == \"stud\":\n",
        "                samples2 = np.random.standard_t(df=3, size=n)\n",
        "            elif dist2 == \"lap\":\n",
        "                beta = 0.70710678118\n",
        "                samples2 = np.random.laplace(loc=0, scale=beta, size=n)\n",
        "            elif dist2 == \"weib\":\n",
        "                samples2 = np.random.weibull(a=1 / 2, size=n) * 0.31622776601\n",
        "            elif dist2 == \"exp\":\n",
        "                samples2 = np.random.exponential(scale=1, size=n)\n",
        "\n",
        "            features2 = extract_multiple_features(samples2, n, k_or_d, dist1)\n",
        "            X.append(features2)\n",
        "            y.append(1)\n",
        "\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y, test_size=0.3, random_state=42\n",
        "        )\n",
        "\n",
        "        n_metrics = []\n",
        "        for name, clf in classifiers.items():\n",
        "            clf.fit(X_train, y_train)\n",
        "            y_pred = clf.predict(X_test)\n",
        "            acc = accuracy_score(y_test, y_pred)\n",
        "            report = classification_report(y_test, y_pred, output_dict=True)\n",
        "            n_metrics.append(\n",
        "                {\n",
        "                    \"Классификатор\": name,\n",
        "                    \"Точность\": acc,\n",
        "                    \"Precision\": report[\"1\"][\"precision\"],\n",
        "                    \"Recall\": report[\"1\"][\"recall\"],\n",
        "                    \"F1-score\": report[\"1\"][\"f1-score\"],\n",
        "                }\n",
        "            )\n",
        "\n",
        "        results.append({\"n\": n, \"Метрики\": pd.DataFrame(n_metrics)})\n",
        "\n",
        "    print(\"\\nРезультаты анализа метрик для различных n:\")\n",
        "    for result in results:\n",
        "        print(f\"\\nРазмер выборки n = {result['n']}\")\n",
        "        print(result[\"Метрики\"])\n",
        "    selected_clf.fit(X, y)\n",
        "    return selected_clf\n",
        "\n",
        "\n",
        "def create_classifier_wrapper(clf):\n",
        "    def wrapper(a, b):\n",
        "        features = np.array([[a, b]])\n",
        "        return clf.predict(features)[0]\n",
        "\n",
        "    return wrapper\n"
      ]
    }
  ]
}